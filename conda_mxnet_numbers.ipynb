{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Setup\n",
    "*Setup input and output object stores\n",
    "*Get execution role to have security access to AWS account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "\n",
    "#Bucket location to save custom code\n",
    "custom_code_upload_location = 's3://kb-sagemaker/customcode/mxnet'\n",
    "\n",
    "#Bucket location where results of model training are saved.\n",
    "model_artifacts_location = 's3://kb-sagemaker/artifacts'\n",
    "\n",
    "#IAM execution role that gives Amazon SageMaker access to resources in your AWS account.\n",
    "#We can use the Amazon SageMaker Python SDK to get the role from our notebook environment. \n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Estimator\n",
    "Estimator server for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:In an upcoming version of the SageMaker Python SDK, framework_version will be required to create an estimator. Please add framework_version=1.2 to your constructor to avoid an error in the future.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.mxnet import MXNet\n",
    "\n",
    "mnist_estimator = MXNet(entry_point='/home/ec2-user/sample-notebooks/sagemaker-python-sdk/mxnet_mnist/mnist.py',\n",
    "                        role=role,\n",
    "                        output_path=model_artifacts_location,\n",
    "                        code_location=custom_code_upload_location,\n",
    "                        train_instance_count=1, \n",
    "                        train_instance_type='ml.m4.xlarge',\n",
    "                        hyperparameters={'learning_rate': 0.1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model\n",
    "Load test data an train model deployed from previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: sagemaker-mxnet-2018-11-08-18-12-32-532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-08 18:12:32 Starting - Starting the training job...\n",
      "2018-11-08 18:12:33 Starting - Launching requested ML instances......\n",
      "2018-11-08 18:13:36 Starting - Preparing the instances for training......\n",
      "2018-11-08 18:15:00 Downloading - Downloading input data..\n",
      "\u001b[31m2018-11-08 18:15:11,425 INFO - root - running container entrypoint\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:11,425 INFO - root - starting train task\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:11,431 INFO - container_support.training - Training starting\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:12,259 WARNING - mxnet_container.train - #033[1;33mThis required structure for training scripts will be deprecated with the next major release of MXNet images. The train() function will no longer be required; instead the training script must be able to be run as a standalone script. For more information, see https://github.com/aws/sagemaker-python-sdk/tree/master/src/sagemaker/mxnet#updating-your-mxnet-training-script.#033[1;0m\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:13,676 INFO - mxnet_container.train - MXNetTrainingEnvironment: {'enable_cloudwatch_metrics': False, 'available_gpus': 0, 'channels': {u'test': {u'TrainingInputMode': u'File', u'RecordWrapperType': u'None', u'S3DistributionType': u'FullyReplicated'}, u'train': {u'TrainingInputMode': u'File', u'RecordWrapperType': u'None', u'S3DistributionType': u'FullyReplicated'}}, '_ps_verbose': 0, 'resource_config': {u'hosts': [u'algo-1'], u'network_interface_name': u'ethwe', u'current_host': u'algo-1'}, 'user_script_name': u'mnist.py', 'input_config_dir': '/opt/ml/input/config', 'channel_dirs': {u'test': u'/opt/ml/input/data/test', u'train': u'/opt/ml/input/data/train'}, 'code_dir': '/opt/ml/code', 'output_data_dir': '/opt/ml/output/data/', 'output_dir': '/opt/ml/output', 'model_dir': '/opt/ml/model', 'hyperparameters': {u'sagemaker_program': u'mnist.py', u'learning_rate': 0.1, u'sagemaker_submit_directory': u's3://kb-sagemaker/customcode/mxnet/sagemaker-mxnet-2018-11-08-18-12-32-532/source/sourcedir.tar.gz', u'sagemaker_region': u'us-west-2', u'sagemaker_enable_cloudwatch_metrics': False, u'sagemaker_job_name': u'sagemaker-mxnet-2018-11-08-18-12-32-532', u'sagemaker_container_log_level': 20}, 'hosts': [u'algo-1'], 'job_name': 'sagemaker-mxnet-2018-11-08-18-12-32-532', '_ps_port': 8000, 'user_script_archive': u's3://kb-sagemaker/customcode/mxnet/sagemaker-mxnet-2018-11-08-18-12-32-532/source/sourcedir.tar.gz', '_scheduler_host': u'algo-1', 'sagemaker_region': u'us-west-2', '_scheduler_ip': '10.32.0.4', 'input_dir': '/opt/ml/input', 'user_requirements_file': None, 'current_host': u'algo-1', 'container_log_level': 20, 'available_cpus': 4, 'base_dir': '/opt/ml'}\u001b[0m\n",
      "\u001b[31mDownloading s3://kb-sagemaker/customcode/mxnet/sagemaker-mxnet-2018-11-08-18-12-32-532/source/sourcedir.tar.gz to /tmp/script.tar.gz\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:14,030 INFO - mxnet_container.train - Starting distributed training task\u001b[0m\n",
      "\u001b[31m/opt/ml/code/mnist.py:13: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  labels = np.fromstring(flbl.read(), dtype=np.int8)\u001b[0m\n",
      "\u001b[31m/opt/ml/code/mnist.py:16: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  images = np.fromstring(fimg.read(), dtype=np.uint8).reshape(len(labels), rows, cols)\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:15,464 INFO - root - Epoch[0] Batch [100]#011Speed: 65421.92 samples/sec#011accuracy=0.113267\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:15,614 INFO - root - Epoch[0] Batch [200]#011Speed: 66603.11 samples/sec#011accuracy=0.112500\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:15,759 INFO - root - Epoch[0] Batch [300]#011Speed: 69469.03 samples/sec#011accuracy=0.113600\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:15,913 INFO - root - Epoch[0] Batch [400]#011Speed: 64820.94 samples/sec#011accuracy=0.107500\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:16,071 INFO - root - Epoch[0] Batch [500]#011Speed: 63719.40 samples/sec#011accuracy=0.115900\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:16,232 INFO - root - Epoch[0] Train-accuracy=0.233939\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:16,232 INFO - root - Epoch[0] Time cost=0.950\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:16,324 INFO - root - Epoch[0] Validation-accuracy=0.335600\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:16,484 INFO - root - Epoch[1] Batch [100]#011Speed: 63087.89 samples/sec#011accuracy=0.461683\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:16,642 INFO - root - Epoch[1] Batch [200]#011Speed: 63633.85 samples/sec#011accuracy=0.664900\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:16,801 INFO - root - Epoch[1] Batch [300]#011Speed: 63110.20 samples/sec#011accuracy=0.772500\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:16,957 INFO - root - Epoch[1] Batch [400]#011Speed: 63795.87 samples/sec#011accuracy=0.798600\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:17,134 INFO - root - Epoch[1] Batch [500]#011Speed: 56652.76 samples/sec#011accuracy=0.824100\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:17,307 INFO - root - Epoch[1] Train-accuracy=0.841515\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:17,307 INFO - root - Epoch[1] Time cost=0.983\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:17,396 INFO - root - Epoch[1] Validation-accuracy=0.838400\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:17,558 INFO - root - Epoch[2] Batch [100]#011Speed: 62178.28 samples/sec#011accuracy=0.860693\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:17,719 INFO - root - Epoch[2] Batch [200]#011Speed: 62399.36 samples/sec#011accuracy=0.871300\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:17,869 INFO - root - Epoch[2] Batch [300]#011Speed: 66819.46 samples/sec#011accuracy=0.886200\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:18,026 INFO - root - Epoch[2] Batch [400]#011Speed: 63741.28 samples/sec#011accuracy=0.895000\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:18,183 INFO - root - Epoch[2] Batch [500]#011Speed: 63750.29 samples/sec#011accuracy=0.905800\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:18,329 INFO - root - Epoch[2] Train-accuracy=0.911212\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:18,330 INFO - root - Epoch[2] Time cost=0.934\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:18,415 INFO - root - Epoch[2] Validation-accuracy=0.910600\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:18,571 INFO - root - Epoch[3] Batch [100]#011Speed: 65049.55 samples/sec#011accuracy=0.921980\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:18,755 INFO - root - Epoch[3] Batch [200]#011Speed: 54249.62 samples/sec#011accuracy=0.923900\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:18,913 INFO - root - Epoch[3] Batch [300]#011Speed: 63553.82 samples/sec#011accuracy=0.928800\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:19,076 INFO - root - Epoch[3] Batch [400]#011Speed: 61605.20 samples/sec#011accuracy=0.929700\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:19,277 INFO - root - Epoch[3] Batch [500]#011Speed: 49617.71 samples/sec#011accuracy=0.937000\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:19,453 INFO - root - Epoch[3] Train-accuracy=0.936364\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:19,453 INFO - root - Epoch[3] Time cost=1.037\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:19,545 INFO - root - Epoch[3] Validation-accuracy=0.937600\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:19,694 INFO - root - Epoch[4] Batch [100]#011Speed: 67622.47 samples/sec#011accuracy=0.943069\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:19,839 INFO - root - Epoch[4] Batch [200]#011Speed: 69457.99 samples/sec#011accuracy=0.943800\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:19,984 INFO - root - Epoch[4] Batch [300]#011Speed: 68835.48 samples/sec#011accuracy=0.947600\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:20,129 INFO - root - Epoch[4] Batch [400]#011Speed: 69458.91 samples/sec#011accuracy=0.947500\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:20,287 INFO - root - Epoch[4] Batch [500]#011Speed: 63032.62 samples/sec#011accuracy=0.951400\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:20,443 INFO - root - Epoch[4] Train-accuracy=0.949697\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:20,444 INFO - root - Epoch[4] Time cost=0.899\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:20,535 INFO - root - Epoch[4] Validation-accuracy=0.950500\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:20,686 INFO - root - Epoch[5] Batch [100]#011Speed: 67166.36 samples/sec#011accuracy=0.953960\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:20,830 INFO - root - Epoch[5] Batch [200]#011Speed: 69464.20 samples/sec#011accuracy=0.954500\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:20,975 INFO - root - Epoch[5] Batch [300]#011Speed: 69423.73 samples/sec#011accuracy=0.958300\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:21,119 INFO - root - Epoch[5] Batch [400]#011Speed: 69292.86 samples/sec#011accuracy=0.957800\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:21,281 INFO - root - Epoch[5] Batch [500]#011Speed: 61896.49 samples/sec#011accuracy=0.961800\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:21,444 INFO - root - Epoch[5] Train-accuracy=0.957778\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:21,444 INFO - root - Epoch[5] Time cost=0.909\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:21,528 INFO - root - Epoch[5] Validation-accuracy=0.956200\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:21,678 INFO - root - Epoch[6] Batch [100]#011Speed: 67550.70 samples/sec#011accuracy=0.963168\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:21,824 INFO - root - Epoch[6] Batch [200]#011Speed: 68627.68 samples/sec#011accuracy=0.962500\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:21,966 INFO - root - Epoch[6] Batch [300]#011Speed: 70516.33 samples/sec#011accuracy=0.964600\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2018-11-08 18:15:11 Training - Training image download completed. Training in progress.\u001b[31m2018-11-08 18:15:22,109 INFO - root - Epoch[6] Batch [400]#011Speed: 69968.74 samples/sec#011accuracy=0.966400\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:22,261 INFO - root - Epoch[6] Batch [500]#011Speed: 65800.75 samples/sec#011accuracy=0.967200\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:22,406 INFO - root - Epoch[6] Train-accuracy=0.964040\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:22,407 INFO - root - Epoch[6] Time cost=0.878\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:22,491 INFO - root - Epoch[6] Validation-accuracy=0.960500\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:22,648 INFO - root - Epoch[7] Batch [100]#011Speed: 64265.35 samples/sec#011accuracy=0.967426\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:22,797 INFO - root - Epoch[7] Batch [200]#011Speed: 67063.69 samples/sec#011accuracy=0.968600\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:22,941 INFO - root - Epoch[7] Batch [300]#011Speed: 69626.68 samples/sec#011accuracy=0.969800\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:23,094 INFO - root - Epoch[7] Batch [400]#011Speed: 65424.47 samples/sec#011accuracy=0.971800\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:23,254 INFO - root - Epoch[7] Batch [500]#011Speed: 62529.69 samples/sec#011accuracy=0.972600\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:23,421 INFO - root - Epoch[7] Train-accuracy=0.970303\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:23,422 INFO - root - Epoch[7] Time cost=0.931\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:23,524 INFO - root - Epoch[7] Validation-accuracy=0.963200\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:23,683 INFO - root - Epoch[8] Batch [100]#011Speed: 63747.00 samples/sec#011accuracy=0.971287\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:23,827 INFO - root - Epoch[8] Batch [200]#011Speed: 69350.15 samples/sec#011accuracy=0.973000\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:23,979 INFO - root - Epoch[8] Batch [300]#011Speed: 66091.27 samples/sec#011accuracy=0.972600\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:24,131 INFO - root - Epoch[8] Batch [400]#011Speed: 65948.69 samples/sec#011accuracy=0.976400\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:24,289 INFO - root - Epoch[8] Batch [500]#011Speed: 63482.83 samples/sec#011accuracy=0.976800\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:24,440 INFO - root - Epoch[8] Train-accuracy=0.973131\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:24,440 INFO - root - Epoch[8] Time cost=0.915\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:24,526 INFO - root - Epoch[8] Validation-accuracy=0.964400\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:24,694 INFO - root - Epoch[9] Batch [100]#011Speed: 60033.38 samples/sec#011accuracy=0.974851\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:24,849 INFO - root - Epoch[9] Batch [200]#011Speed: 64606.97 samples/sec#011accuracy=0.976300\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:25,007 INFO - root - Epoch[9] Batch [300]#011Speed: 63461.89 samples/sec#011accuracy=0.975700\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:25,165 INFO - root - Epoch[9] Batch [400]#011Speed: 63396.28 samples/sec#011accuracy=0.979000\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:25,324 INFO - root - Epoch[9] Batch [500]#011Speed: 62998.92 samples/sec#011accuracy=0.979200\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:25,481 INFO - root - Epoch[9] Train-accuracy=0.975960\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:25,481 INFO - root - Epoch[9] Time cost=0.955\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:25,596 INFO - root - Epoch[9] Validation-accuracy=0.966500\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:25,751 INFO - root - Epoch[10] Batch [100]#011Speed: 65150.39 samples/sec#011accuracy=0.978614\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:25,903 INFO - root - Epoch[10] Batch [200]#011Speed: 66099.08 samples/sec#011accuracy=0.979200\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:26,055 INFO - root - Epoch[10] Batch [300]#011Speed: 65765.25 samples/sec#011accuracy=0.978400\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:26,213 INFO - root - Epoch[10] Batch [400]#011Speed: 63811.79 samples/sec#011accuracy=0.982000\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:26,373 INFO - root - Epoch[10] Batch [500]#011Speed: 62452.41 samples/sec#011accuracy=0.982600\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:26,528 INFO - root - Epoch[10] Train-accuracy=0.978990\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:26,529 INFO - root - Epoch[10] Time cost=0.932\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:26,620 INFO - root - Epoch[10] Validation-accuracy=0.967500\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:26,779 INFO - root - Epoch[11] Batch [100]#011Speed: 63512.63 samples/sec#011accuracy=0.980891\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:26,936 INFO - root - Epoch[11] Batch [200]#011Speed: 63580.50 samples/sec#011accuracy=0.982600\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:27,088 INFO - root - Epoch[11] Batch [300]#011Speed: 66328.84 samples/sec#011accuracy=0.981200\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:27,238 INFO - root - Epoch[11] Batch [400]#011Speed: 66768.19 samples/sec#011accuracy=0.983900\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:27,398 INFO - root - Epoch[11] Batch [500]#011Speed: 62501.18 samples/sec#011accuracy=0.985600\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:27,547 INFO - root - Epoch[11] Train-accuracy=0.981818\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:27,547 INFO - root - Epoch[11] Time cost=0.927\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:27,649 INFO - root - Epoch[11] Validation-accuracy=0.968100\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:27,837 INFO - root - Epoch[12] Batch [100]#011Speed: 53772.71 samples/sec#011accuracy=0.982970\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:27,996 INFO - root - Epoch[12] Batch [200]#011Speed: 62986.24 samples/sec#011accuracy=0.984400\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:28,153 INFO - root - Epoch[12] Batch [300]#011Speed: 63420.43 samples/sec#011accuracy=0.984100\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:28,313 INFO - root - Epoch[12] Batch [400]#011Speed: 62734.04 samples/sec#011accuracy=0.985100\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:28,461 INFO - root - Epoch[12] Batch [500]#011Speed: 67935.28 samples/sec#011accuracy=0.987400\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:28,602 INFO - root - Epoch[12] Train-accuracy=0.983434\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:28,602 INFO - root - Epoch[12] Time cost=0.953\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:28,690 INFO - root - Epoch[12] Validation-accuracy=0.969100\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:28,850 INFO - root - Epoch[13] Batch [100]#011Speed: 63478.41 samples/sec#011accuracy=0.985347\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:29,004 INFO - root - Epoch[13] Batch [200]#011Speed: 65001.36 samples/sec#011accuracy=0.986900\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:29,161 INFO - root - Epoch[13] Batch [300]#011Speed: 63737.70 samples/sec#011accuracy=0.985500\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:29,320 INFO - root - Epoch[13] Batch [400]#011Speed: 63121.31 samples/sec#011accuracy=0.986700\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:29,477 INFO - root - Epoch[13] Batch [500]#011Speed: 63687.37 samples/sec#011accuracy=0.988700\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:29,632 INFO - root - Epoch[13] Train-accuracy=0.986162\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:29,633 INFO - root - Epoch[13] Time cost=0.942\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:29,727 INFO - root - Epoch[13] Validation-accuracy=0.969900\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:29,907 INFO - root - Epoch[14] Batch [100]#011Speed: 56250.53 samples/sec#011accuracy=0.986931\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:30,064 INFO - root - Epoch[14] Batch [200]#011Speed: 63737.31 samples/sec#011accuracy=0.988500\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:30,222 INFO - root - Epoch[14] Batch [300]#011Speed: 63654.90 samples/sec#011accuracy=0.988000\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:30,384 INFO - root - Epoch[14] Batch [400]#011Speed: 61830.61 samples/sec#011accuracy=0.988400\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:30,541 INFO - root - Epoch[14] Batch [500]#011Speed: 63657.80 samples/sec#011accuracy=0.989800\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:30,684 INFO - root - Epoch[14] Train-accuracy=0.987778\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:30,684 INFO - root - Epoch[14] Time cost=0.957\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:30,769 INFO - root - Epoch[14] Validation-accuracy=0.970000\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:30,930 INFO - root - Epoch[15] Batch [100]#011Speed: 62467.20 samples/sec#011accuracy=0.988614\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:31,079 INFO - root - Epoch[15] Batch [200]#011Speed: 67415.42 samples/sec#011accuracy=0.989400\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:31,237 INFO - root - Epoch[15] Batch [300]#011Speed: 63501.28 samples/sec#011accuracy=0.989700\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:31,389 INFO - root - Epoch[15] Batch [400]#011Speed: 65642.26 samples/sec#011accuracy=0.990300\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:31,534 INFO - root - Epoch[15] Batch [500]#011Speed: 69367.81 samples/sec#011accuracy=0.992200\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:31,677 INFO - root - Epoch[15] Train-accuracy=0.989192\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:31,677 INFO - root - Epoch[15] Time cost=0.908\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:31,762 INFO - root - Epoch[15] Validation-accuracy=0.970500\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:31,940 INFO - root - Epoch[16] Batch [100]#011Speed: 56726.31 samples/sec#011accuracy=0.991089\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m2018-11-08 18:15:32,084 INFO - root - Epoch[16] Batch [200]#011Speed: 69319.78 samples/sec#011accuracy=0.990900\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:32,233 INFO - root - Epoch[16] Batch [300]#011Speed: 67422.78 samples/sec#011accuracy=0.991500\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:32,392 INFO - root - Epoch[16] Batch [400]#011Speed: 62923.97 samples/sec#011accuracy=0.991200\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:32,547 INFO - root - Epoch[16] Batch [500]#011Speed: 64494.12 samples/sec#011accuracy=0.992600\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:32,703 INFO - root - Epoch[16] Train-accuracy=0.991414\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:32,703 INFO - root - Epoch[16] Time cost=0.941\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:32,792 INFO - root - Epoch[16] Validation-accuracy=0.971000\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:32,960 INFO - root - Epoch[17] Batch [100]#011Speed: 60119.00 samples/sec#011accuracy=0.992079\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:33,117 INFO - root - Epoch[17] Batch [200]#011Speed: 63609.62 samples/sec#011accuracy=0.992300\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:33,275 INFO - root - Epoch[17] Batch [300]#011Speed: 63470.24 samples/sec#011accuracy=0.992500\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:33,432 INFO - root - Epoch[17] Batch [400]#011Speed: 63747.00 samples/sec#011accuracy=0.992600\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:33,578 INFO - root - Epoch[17] Batch [500]#011Speed: 68471.05 samples/sec#011accuracy=0.994300\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:33,721 INFO - root - Epoch[17] Train-accuracy=0.991717\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:33,721 INFO - root - Epoch[17] Time cost=0.930\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:33,810 INFO - root - Epoch[17] Validation-accuracy=0.970800\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:33,993 INFO - root - Epoch[18] Batch [100]#011Speed: 54871.54 samples/sec#011accuracy=0.993960\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:34,148 INFO - root - Epoch[18] Batch [200]#011Speed: 64597.42 samples/sec#011accuracy=0.993400\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:34,292 INFO - root - Epoch[18] Batch [300]#011Speed: 69586.59 samples/sec#011accuracy=0.993800\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:34,448 INFO - root - Epoch[18] Batch [400]#011Speed: 64357.85 samples/sec#011accuracy=0.993400\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:34,605 INFO - root - Epoch[18] Batch [500]#011Speed: 63796.26 samples/sec#011accuracy=0.994600\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:34,760 INFO - root - Epoch[18] Train-accuracy=0.993131\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:34,760 INFO - root - Epoch[18] Time cost=0.950\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:34,849 INFO - root - Epoch[18] Validation-accuracy=0.971900\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:35,007 INFO - root - Epoch[19] Batch [100]#011Speed: 64206.72 samples/sec#011accuracy=0.995149\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:35,163 INFO - root - Epoch[19] Batch [200]#011Speed: 64184.02 samples/sec#011accuracy=0.994300\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:35,319 INFO - root - Epoch[19] Batch [300]#011Speed: 63898.21 samples/sec#011accuracy=0.994900\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:35,480 INFO - root - Epoch[19] Batch [400]#011Speed: 62384.97 samples/sec#011accuracy=0.994200\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:35,637 INFO - root - Epoch[19] Batch [500]#011Speed: 63905.12 samples/sec#011accuracy=0.995600\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:35,792 INFO - root - Epoch[19] Train-accuracy=0.994545\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:35,792 INFO - root - Epoch[19] Time cost=0.943\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:35,884 INFO - root - Epoch[19] Validation-accuracy=0.972300\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:36,031 INFO - root - Epoch[20] Batch [100]#011Speed: 68789.98 samples/sec#011accuracy=0.996139\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:36,218 INFO - root - Epoch[20] Batch [200]#011Speed: 53712.32 samples/sec#011accuracy=0.995000\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:36,376 INFO - root - Epoch[20] Batch [300]#011Speed: 63170.37 samples/sec#011accuracy=0.995700\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:36,527 INFO - root - Epoch[20] Batch [400]#011Speed: 66449.16 samples/sec#011accuracy=0.994900\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:36,671 INFO - root - Epoch[20] Batch [500]#011Speed: 69309.24 samples/sec#011accuracy=0.996700\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:36,814 INFO - root - Epoch[20] Train-accuracy=0.995354\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:36,815 INFO - root - Epoch[20] Time cost=0.931\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:36,899 INFO - root - Epoch[20] Validation-accuracy=0.973000\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:37,057 INFO - root - Epoch[21] Batch [100]#011Speed: 63736.83 samples/sec#011accuracy=0.997030\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:37,206 INFO - root - Epoch[21] Batch [200]#011Speed: 67463.67 samples/sec#011accuracy=0.995700\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:37,349 INFO - root - Epoch[21] Batch [300]#011Speed: 69978.54 samples/sec#011accuracy=0.996800\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:37,508 INFO - root - Epoch[21] Batch [400]#011Speed: 62884.06 samples/sec#011accuracy=0.995600\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:37,666 INFO - root - Epoch[21] Batch [500]#011Speed: 63694.72 samples/sec#011accuracy=0.997300\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:37,821 INFO - root - Epoch[21] Train-accuracy=0.996061\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:37,821 INFO - root - Epoch[21] Time cost=0.922\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:37,912 INFO - root - Epoch[21] Validation-accuracy=0.973800\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:38,077 INFO - root - Epoch[22] Batch [100]#011Speed: 61301.95 samples/sec#011accuracy=0.998119\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:38,253 INFO - root - Epoch[22] Batch [200]#011Speed: 56843.32 samples/sec#011accuracy=0.996500\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:38,430 INFO - root - Epoch[22] Batch [300]#011Speed: 56446.75 samples/sec#011accuracy=0.997500\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:38,585 INFO - root - Epoch[22] Batch [400]#011Speed: 64593.64 samples/sec#011accuracy=0.996100\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:38,730 INFO - root - Epoch[22] Batch [500]#011Speed: 69527.99 samples/sec#011accuracy=0.998000\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:38,873 INFO - root - Epoch[22] Train-accuracy=0.996667\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:38,873 INFO - root - Epoch[22] Time cost=0.961\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:38,963 INFO - root - Epoch[22] Validation-accuracy=0.974000\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:39,122 INFO - root - Epoch[23] Batch [100]#011Speed: 63600.27 samples/sec#011accuracy=0.998317\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:39,280 INFO - root - Epoch[23] Batch [200]#011Speed: 63588.50 samples/sec#011accuracy=0.996700\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:39,434 INFO - root - Epoch[23] Batch [300]#011Speed: 64760.09 samples/sec#011accuracy=0.997800\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:39,591 INFO - root - Epoch[23] Batch [400]#011Speed: 63740.99 samples/sec#011accuracy=0.996800\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:39,748 INFO - root - Epoch[23] Batch [500]#011Speed: 63883.03 samples/sec#011accuracy=0.998400\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:39,907 INFO - root - Epoch[23] Train-accuracy=0.997576\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:39,907 INFO - root - Epoch[23] Time cost=0.944\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:39,997 INFO - root - Epoch[23] Validation-accuracy=0.974500\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:40,155 INFO - root - Epoch[24] Batch [100]#011Speed: 64031.95 samples/sec#011accuracy=0.998713\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:40,323 INFO - root - Epoch[24] Batch [200]#011Speed: 59431.12 samples/sec#011accuracy=0.997300\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:40,499 INFO - root - Epoch[24] Batch [300]#011Speed: 56809.14 samples/sec#011accuracy=0.998300\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:40,656 INFO - root - Epoch[24] Batch [400]#011Speed: 63798.29 samples/sec#011accuracy=0.997100\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:40,814 INFO - root - Epoch[24] Batch [500]#011Speed: 63705.95 samples/sec#011accuracy=0.998800\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:40,969 INFO - root - Epoch[24] Train-accuracy=0.998081\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:40,969 INFO - root - Epoch[24] Time cost=0.972\u001b[0m\n",
      "\u001b[31m2018-11-08 18:15:41,057 INFO - root - Epoch[24] Validation-accuracy=0.975000\u001b[0m\n",
      "\n",
      "2018-11-08 18:15:48 Uploading - Uploading generated training model\n",
      "2018-11-08 18:15:48 Completed - Training job completed\n",
      "Billable seconds: 49\n",
      "CPU times: user 453 ms, sys: 0 ns, total: 453 ms\n",
      "Wall time: 3min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import boto3\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "train_data_location = 's3://sagemaker-sample-data-{}/mxnet/mnist/train'.format(region)\n",
    "test_data_location = 's3://sagemaker-sample-data-{}/mxnet/mnist/test'.format(region)\n",
    "\n",
    "mnist_estimator.fit({'train': train_data_location, 'test': test_data_location})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Trained Model to Endpoint for Predictions\n",
    "Create server instance to host trained model and act as an endpoint for prediction API calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: sagemaker-mxnet-2018-11-08-18-12-32-532\n",
      "INFO:sagemaker:Creating endpoint with name sagemaker-mxnet-2018-11-08-18-12-32-532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------!CPU times: user 300 ms, sys: 0 ns, total: 300 ms\n",
      "Wall time: 5min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "predictor = mnist_estimator.deploy(initial_instance_count=1,\n",
    "                                   instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup HTML Drawing Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"text/Javascript\">\n",
       "    var pixels = [];\n",
       "    for (var i = 0; i < 28*28; i++) pixels[i] = 0;\n",
       "    var click = 0;\n",
       "\n",
       "    var canvas = document.querySelector(\"canvas\");\n",
       "    canvas.addEventListener(\"mousemove\", function(e){\n",
       "        if (e.buttons == 1) {\n",
       "            click = 1;\n",
       "            canvas.getContext(\"2d\").fillStyle = \"rgb(0,0,0)\";\n",
       "            canvas.getContext(\"2d\").fillRect(e.offsetX, e.offsetY, 8, 8);\n",
       "            x = Math.floor(e.offsetY * 0.2);\n",
       "            y = Math.floor(e.offsetX * 0.2) + 1;\n",
       "            for (var dy = 0; dy < 2; dy++){\n",
       "                for (var dx = 0; dx < 2; dx++){\n",
       "                    if ((x + dx < 28) && (y + dy < 28)){\n",
       "                        pixels[(y+dy)+(x+dx)*28] = 1;\n",
       "                    }\n",
       "                }\n",
       "            }\n",
       "        } else {\n",
       "            if (click == 1) set_value();\n",
       "            click = 0;\n",
       "        }\n",
       "    });\n",
       "    function clear_value(){\n",
       "        canvas.getContext(\"2d\").fillStyle = \"rgb(255,255,255)\";\n",
       "        canvas.getContext(\"2d\").fillRect(0, 0, 140, 140);\n",
       "        for (var i = 0; i < 28*28; i++) pixels[i] = 0;\n",
       "    }\n",
       "    \n",
       "    function set_value(){\n",
       "        var result = \"[[\"\n",
       "        for (var i = 0; i < 28; i++) {\n",
       "            result += \"[\"\n",
       "            for (var j = 0; j < 28; j++) {\n",
       "                result += pixels [i * 28 + j]\n",
       "                if (j < 27) {\n",
       "                    result += \", \"\n",
       "                }\n",
       "            }\n",
       "            result += \"]\"\n",
       "            if (i < 27) {\n",
       "                result += \", \"\n",
       "            }\n",
       "        }\n",
       "        result += \"]]\"\n",
       "        var kernel = IPython.notebook.kernel;\n",
       "        kernel.execute(\"data = \" + result)\n",
       "    }\n",
       "</script>\n",
       "<table>\n",
       "<td style=\"border-style: none;\">\n",
       "<div style=\"border: solid 2px #666; width: 143px; height: 144px;\">\n",
       "<canvas width=\"140\" height=\"140\"></canvas>\n",
       "</div></td>\n",
       "<td style=\"border-style: none;\">\n",
       "<button onclick=\"clear_value()\">Clear</button>\n",
       "</td>\n",
       "</table>\n",
       "\n",
       "<!-- This work has been modified from the original and is licensed under the Apache 2.0 License. -->\n",
       "\n",
       "<!--\n",
       "                                     Apache License\n",
       "                           Version 2.0, January 2004\n",
       "                        http://www.apache.org/licenses/\n",
       "\n",
       "   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n",
       "\n",
       "   1. Definitions.\n",
       "\n",
       "      \"License\" shall mean the terms and conditions for use, reproduction,\n",
       "      and distribution as defined by Sections 1 through 9 of this document.\n",
       "\n",
       "      \"Licensor\" shall mean the copyright owner or entity authorized by\n",
       "      the copyright owner that is granting the License.\n",
       "\n",
       "      \"Legal Entity\" shall mean the union of the acting entity and all\n",
       "      other entities that control, are controlled by, or are under common\n",
       "      control with that entity. For the purposes of this definition,\n",
       "      \"control\" means (i) the power, direct or indirect, to cause the\n",
       "      direction or management of such entity, whether by contract or\n",
       "      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n",
       "      outstanding shares, or (iii) beneficial ownership of such entity.\n",
       "\n",
       "      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n",
       "      exercising permissions granted by this License.\n",
       "\n",
       "      \"Source\" form shall mean the preferred form for making modifications,\n",
       "      including but not limited to software source code, documentation\n",
       "      source, and configuration files.\n",
       "\n",
       "      \"Object\" form shall mean any form resulting from mechanical\n",
       "      transformation or translation of a Source form, including but\n",
       "      not limited to compiled object code, generated documentation,\n",
       "      and conversions to other media types.\n",
       "\n",
       "      \"Work\" shall mean the work of authorship, whether in Source or\n",
       "      Object form, made available under the License, as indicated by a\n",
       "      copyright notice that is included in or attached to the work\n",
       "      (an example is provided in the Appendix below).\n",
       "\n",
       "      \"Derivative Works\" shall mean any work, whether in Source or Object\n",
       "      form, that is based on (or derived from) the Work and for which the\n",
       "      editorial revisions, annotations, elaborations, or other modifications\n",
       "      represent, as a whole, an original work of authorship. For the purposes\n",
       "      of this License, Derivative Works shall not include works that remain\n",
       "      separable from, or merely link (or bind by name) to the interfaces of,\n",
       "      the Work and Derivative Works thereof.\n",
       "\n",
       "      \"Contribution\" shall mean any work of authorship, including\n",
       "      the original version of the Work and any modifications or additions\n",
       "      to that Work or Derivative Works thereof, that is intentionally\n",
       "      submitted to Licensor for inclusion in the Work by the copyright owner\n",
       "      or by an individual or Legal Entity authorized to submit on behalf of\n",
       "      the copyright owner. For the purposes of this definition, \"submitted\"\n",
       "      means any form of electronic, verbal, or written communication sent\n",
       "      to the Licensor or its representatives, including but not limited to\n",
       "      communication on electronic mailing lists, source code control systems,\n",
       "      and issue tracking systems that are managed by, or on behalf of, the\n",
       "      Licensor for the purpose of discussing and improving the Work, but\n",
       "      excluding communication that is conspicuously marked or otherwise\n",
       "      designated in writing by the copyright owner as \"Not a Contribution.\"\n",
       "\n",
       "      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n",
       "      on behalf of whom a Contribution has been received by Licensor and\n",
       "      subsequently incorporated within the Work.\n",
       "\n",
       "   2. Grant of Copyright License. Subject to the terms and conditions of\n",
       "      this License, each Contributor hereby grants to You a perpetual,\n",
       "      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n",
       "      copyright license to reproduce, prepare Derivative Works of,\n",
       "      publicly display, publicly perform, sublicense, and distribute the\n",
       "      Work and such Derivative Works in Source or Object form.\n",
       "\n",
       "   3. Grant of Patent License. Subject to the terms and conditions of\n",
       "      this License, each Contributor hereby grants to You a perpetual,\n",
       "      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n",
       "      (except as stated in this section) patent license to make, have made,\n",
       "      use, offer to sell, sell, import, and otherwise transfer the Work,\n",
       "      where such license applies only to those patent claims licensable\n",
       "      by such Contributor that are necessarily infringed by their\n",
       "      Contribution(s) alone or by combination of their Contribution(s)\n",
       "      with the Work to which such Contribution(s) was submitted. If You\n",
       "      institute patent litigation against any entity (including a\n",
       "      cross-claim or counterclaim in a lawsuit) alleging that the Work\n",
       "      or a Contribution incorporated within the Work constitutes direct\n",
       "      or contributory patent infringement, then any patent licenses\n",
       "      granted to You under this License for that Work shall terminate\n",
       "      as of the date such litigation is filed.\n",
       "\n",
       "   4. Redistribution. You may reproduce and distribute copies of the\n",
       "      Work or Derivative Works thereof in any medium, with or without\n",
       "      modifications, and in Source or Object form, provided that You\n",
       "      meet the following conditions:\n",
       "\n",
       "      (a) You must give any other recipients of the Work or\n",
       "          Derivative Works a copy of this License; and\n",
       "\n",
       "      (b) You must cause any modified files to carry prominent notices\n",
       "          stating that You changed the files; and\n",
       "\n",
       "      (c) You must retain, in the Source form of any Derivative Works\n",
       "          that You distribute, all copyright, patent, trademark, and\n",
       "          attribution notices from the Source form of the Work,\n",
       "          excluding those notices that do not pertain to any part of\n",
       "          the Derivative Works; and\n",
       "\n",
       "      (d) If the Work includes a \"NOTICE\" text file as part of its\n",
       "          distribution, then any Derivative Works that You distribute must\n",
       "          include a readable copy of the attribution notices contained\n",
       "          within such NOTICE file, excluding those notices that do not\n",
       "          pertain to any part of the Derivative Works, in at least one\n",
       "          of the following places: within a NOTICE text file distributed\n",
       "          as part of the Derivative Works; within the Source form or\n",
       "          documentation, if provided along with the Derivative Works; or,\n",
       "          within a display generated by the Derivative Works, if and\n",
       "          wherever such third-party notices normally appear. The contents\n",
       "          of the NOTICE file are for informational purposes only and\n",
       "          do not modify the License. You may add Your own attribution\n",
       "          notices within Derivative Works that You distribute, alongside\n",
       "          or as an addendum to the NOTICE text from the Work, provided\n",
       "          that such additional attribution notices cannot be construed\n",
       "          as modifying the License.\n",
       "\n",
       "      You may add Your own copyright statement to Your modifications and\n",
       "      may provide additional or different license terms and conditions\n",
       "      for use, reproduction, or distribution of Your modifications, or\n",
       "      for any such Derivative Works as a whole, provided Your use,\n",
       "      reproduction, and distribution of the Work otherwise complies with\n",
       "      the conditions stated in this License.\n",
       "\n",
       "   5. Submission of Contributions. Unless You explicitly state otherwise,\n",
       "      any Contribution intentionally submitted for inclusion in the Work\n",
       "      by You to the Licensor shall be under the terms and conditions of\n",
       "      this License, without any additional terms or conditions.\n",
       "      Notwithstanding the above, nothing herein shall supersede or modify\n",
       "      the terms of any separate license agreement you may have executed\n",
       "      with Licensor regarding such Contributions.\n",
       "\n",
       "   6. Trademarks. This License does not grant permission to use the trade\n",
       "      names, trademarks, service marks, or product names of the Licensor,\n",
       "      except as required for reasonable and customary use in describing the\n",
       "      origin of the Work and reproducing the content of the NOTICE file.\n",
       "\n",
       "   7. Disclaimer of Warranty. Unless required by applicable law or\n",
       "      agreed to in writing, Licensor provides the Work (and each\n",
       "      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n",
       "      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n",
       "      implied, including, without limitation, any warranties or conditions\n",
       "      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n",
       "      PARTICULAR PURPOSE. You are solely responsible for determining the\n",
       "      appropriateness of using or redistributing the Work and assume any\n",
       "      risks associated with Your exercise of permissions under this License.\n",
       "\n",
       "   8. Limitation of Liability. In no event and under no legal theory,\n",
       "      whether in tort (including negligence), contract, or otherwise,\n",
       "      unless required by applicable law (such as deliberate and grossly\n",
       "      negligent acts) or agreed to in writing, shall any Contributor be\n",
       "      liable to You for damages, including any direct, indirect, special,\n",
       "      incidental, or consequential damages of any character arising as a\n",
       "      result of this License or out of the use or inability to use the\n",
       "      Work (including but not limited to damages for loss of goodwill,\n",
       "      work stoppage, computer failure or malfunction, or any and all\n",
       "      other commercial damages or losses), even if such Contributor\n",
       "      has been advised of the possibility of such damages.\n",
       "\n",
       "   9. Accepting Warranty or Additional Liability. While redistributing\n",
       "      the Work or Derivative Works thereof, You may choose to offer,\n",
       "      and charge a fee for, acceptance of support, warranty, indemnity,\n",
       "      or other liability obligations and/or rights consistent with this\n",
       "      License. However, in accepting such obligations, You may act only\n",
       "      on Your own behalf and on Your sole responsibility, not on behalf\n",
       "      of any other Contributor, and only if You agree to indemnify,\n",
       "      defend, and hold each Contributor harmless for any liability\n",
       "      incurred by, or claims asserted against, such Contributor by reason\n",
       "      of your accepting any such warranty or additional liability.\n",
       "\n",
       "   END OF TERMS AND CONDITIONS\n",
       "\n",
       "   APPENDIX: How to apply the Apache License to your work.\n",
       "\n",
       "      To apply the Apache License to your work, attach the following\n",
       "      boilerplate notice, with the fields enclosed by brackets \"{}\"\n",
       "      replaced with your own identifying information. (Don't include\n",
       "      the brackets!)  The text should be enclosed in the appropriate\n",
       "      comment syntax for the file format. We also recommend that a\n",
       "      file or class name and description of purpose be included on the\n",
       "      same \"printed page\" as the copyright notice for easier\n",
       "      identification within third-party archives.\n",
       "\n",
       "   Copyright {yyyy} {name of copyright owner}\n",
       "\n",
       "   Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "   you may not use this file except in compliance with the License.\n",
       "   You may obtain a copy of the License at\n",
       "\n",
       "       http://www.apache.org/licenses/LICENSE-2.0\n",
       "\n",
       "   Unless required by applicable law or agreed to in writing, software\n",
       "   distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "   See the License for the specific language governing permissions and\n",
       "   limitations under the License.\n",
       "-->\n",
       "\n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(open(\"/home/ec2-user/sample-notebooks/sagemaker-python-sdk/mxnet_mnist/input.html\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make inference for above drawing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw prediction result:\n",
      "[[1.162921433548467e-14, 0.0004702737496700138, 0.9995296001434326, 6.82744314417505e-08, 5.473607373287278e-16, 9.93337665980865e-16, 1.7941066384266624e-12, 2.0634491448845438e-08, 9.843352799521199e-09, 2.5989752216571644e-18]]\n",
      "Labeled predictions: \n",
      "[(0, 1.162921433548467e-14), (1, 0.0004702737496700138), (2, 0.9995296001434326), (3, 6.82744314417505e-08), (4, 5.473607373287278e-16), (5, 9.93337665980865e-16), (6, 1.7941066384266624e-12), (7, 2.0634491448845438e-08), (8, 9.843352799521199e-09), (9, 2.5989752216571644e-18)]\n",
      "Most likely answer: (2, 0.9995296001434326)\n"
     ]
    }
   ],
   "source": [
    "response = predictor.predict(data)\n",
    "print('Raw prediction result:')\n",
    "print(response)\n",
    "\n",
    "labeled_predictions = list(zip(range(10), response[0]))\n",
    "print('Labeled predictions: ')\n",
    "print(labeled_predictions)\n",
    "\n",
    "labeled_predictions.sort(key=lambda label_and_prob: 1.0 - label_and_prob[1])\n",
    "print('Most likely answer: {}'.format(labeled_predictions[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]]\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

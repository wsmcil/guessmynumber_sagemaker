{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass classification with Amazon SageMaker XGBoost algorithm\n",
    "_**Single machine and distributed training for multiclass classification with Amazon SageMaker XGBoost algorithm**_\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "## Contents\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Prerequisites and Preprocessing](#Prequisites-and-Preprocessing)\n",
    "  1. [Permissions and environment variables](#Permissions-and-environment-variables)\n",
    "  2. [Data ingestion](#Data-ingestion)\n",
    "  3. [Data conversion](#Data-conversion)\n",
    "3. [Training the XGBoost model](#Training-the-XGBoost-model)\n",
    "  1. [Training on a single instance](#Training-on-a-single-instance)\n",
    "  2. [Training on multiple instances](#Training-on-multiple-instances)\n",
    "4. [Set up hosting for the model](#Set-up-hosting-for-the-model)\n",
    "  1. [Import model into hosting](#Import-model-into-hosting)\n",
    "  2. [Create endpoint configuration](#Create-endpoint-configuration)\n",
    "  3. [Create endpoint](#Create-endpoint)\n",
    "5. [Validate the model for use](#Validate-the-model-for-use)\n",
    "\n",
    "---\n",
    "## Introduction\n",
    "\n",
    "\n",
    "This notebook demonstrates the use of Amazon SageMaker’s implementation of the XGBoost algorithm to train and host a multiclass classification model. The MNIST dataset is used for training. It has a training set of 60,000 examples and a test set of 10,000 examples. To illustrate the use of libsvm training data format, we download the dataset and convert it to the libsvm format before training.\n",
    "\n",
    "To get started, we need to set up the environment with a few prerequisites for permissions and configurations.\n",
    "\n",
    "---\n",
    "## Prequisites and Preprocessing\n",
    "\n",
    "### Permissions and environment variables\n",
    "\n",
    "Here we set up the linkage and authentication to AWS services.\n",
    "\n",
    "1. The roles used to give learning and hosting access to your data. See the documentation for how to specify these.\n",
    "2. The S3 bucket that you want to use for training and model data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "isConfigCell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 721 ms, sys: 175 ms, total: 896 ms\n",
      "Wall time: 9.45 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import os\n",
    "import boto3\n",
    "import re\n",
    "import copy\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "bucket='kb-sagemaker' # put your s3 bucket name here, and create s3 bucket\n",
    "prefix = 'sagemaker/DEMO-xgboost-multiclass-classification'\n",
    "# customize to your bucket where you have stored the data\n",
    "bucket_path = 'https://s3-{}.amazonaws.com/{}'.format(region,bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data ingestion\n",
    "\n",
    "Next, we read the dataset from the existing repository into memory, for preprocessing prior to training. This processing could be done *in situ* by Amazon Athena, Apache Spark in Amazon EMR, Amazon Redshift, etc., assuming the dataset is present in the appropriate location. Then, the next step would be to transfer the data to S3 for use in training. For small datasets, such as this one, reading into memory isn't onerous, though it would be for larger datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 828 ms, sys: 312 ms, total: 1.14 s\n",
      "Wall time: 3.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pickle, gzip, numpy, urllib.request, json\n",
    "\n",
    "# Load the dataset\n",
    "urllib.request.urlretrieve(\"http://deeplearning.net/data/mnist/mnist.pkl.gz\", \"mnist.pkl.gz\")\n",
    "f = gzip.open('mnist.pkl.gz', 'rb')\n",
    "train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data conversion\n",
    "\n",
    "Since algorithms have particular input and output requirements, converting the dataset is also part of the process that a data scientist goes through prior to initiating training. In this particular case, the data is converted from pickle-ized numpy array to the libsvm format before being uploaded to S3. The hosted implementation of xgboost consumes the libsvm converted data from S3 for training. The following provides functions for data conversions and file upload to S3 and download from S3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9 µs, sys: 0 ns, total: 9 µs\n",
      "Wall time: 11.7 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import struct\n",
    "import io\n",
    "import boto3\n",
    "\n",
    " \n",
    "def to_libsvm(f, labels, values):\n",
    "     f.write(bytes('\\n'.join(\n",
    "         ['{} {}'.format(label, ' '.join(['{}:{}'.format(i + 1, el) for i, el in enumerate(vec)])) for label, vec in\n",
    "          zip(labels, values)]), 'utf-8'))\n",
    "     return f\n",
    "\n",
    "\n",
    "def write_to_s3(fobj, bucket, key):\n",
    "    return boto3.Session().resource('s3').Bucket(bucket).Object(key).upload_fileobj(fobj)\n",
    "\n",
    "def get_dataset():\n",
    "  import pickle\n",
    "  import gzip\n",
    "  with gzip.open('mnist.pkl.gz', 'rb') as f:\n",
    "      u = pickle._Unpickler(f)\n",
    "      u.encoding = 'latin1'\n",
    "      return u.load()\n",
    "\n",
    "def upload_to_s3(partition_name, partition):\n",
    "    labels = [t.tolist() for t in partition[1]]\n",
    "    vectors = [t.tolist() for t in partition[0]]\n",
    "    num_partition = 5                                 # partition file into 5 parts\n",
    "    partition_bound = int(len(labels)/num_partition)\n",
    "    for i in range(num_partition):\n",
    "        f = io.BytesIO()\n",
    "        to_libsvm(f, labels[i*partition_bound:(i+1)*partition_bound], vectors[i*partition_bound:(i+1)*partition_bound])\n",
    "        f.seek(0)\n",
    "        key = \"{}/{}/examples{}\".format(prefix,partition_name,str(i))\n",
    "        url = 's3n://{}/{}'.format(bucket, key)\n",
    "        print('Writing to {}'.format(url))\n",
    "        write_to_s3(f, bucket, key)\n",
    "        print('Done writing to {}'.format(url))\n",
    "\n",
    "def download_from_s3(partition_name, number, filename):\n",
    "    key = \"{}/{}/examples{}\".format(prefix,partition_name, number)\n",
    "    url = 's3n://{}/{}'.format(bucket, key)\n",
    "    print('Reading from {}'.format(url))\n",
    "    s3 = boto3.resource('s3')\n",
    "    s3.Bucket(bucket).download_file(key, filename)\n",
    "    try:\n",
    "        s3.Bucket(bucket).download_file(key, 'mnist.local.test')\n",
    "    except botocore.exceptions.ClientError as e:\n",
    "        if e.response['Error']['Code'] == \"404\":\n",
    "            print('The object does not exist at {}.'.format(url))\n",
    "        else:\n",
    "            raise        \n",
    "        \n",
    "def convert_data():\n",
    "    train_set, valid_set, test_set = get_dataset()\n",
    "    partitions = [('train', train_set), ('validation', valid_set), ('test', test_set)]\n",
    "    for partition_name, partition in partitions:\n",
    "        print('{}: {} {}'.format(partition_name, partition[0].shape, partition[1].shape))\n",
    "        upload_to_s3(partition_name, partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (50000, 784) (50000,)\n",
      "Writing to s3n://kb-sagemaker/sagemaker/DEMO-xgboost-multiclass-classification/train/examples0\n",
      "Done writing to s3n://kb-sagemaker/sagemaker/DEMO-xgboost-multiclass-classification/train/examples0\n",
      "Writing to s3n://kb-sagemaker/sagemaker/DEMO-xgboost-multiclass-classification/train/examples1\n",
      "Done writing to s3n://kb-sagemaker/sagemaker/DEMO-xgboost-multiclass-classification/train/examples1\n",
      "Writing to s3n://kb-sagemaker/sagemaker/DEMO-xgboost-multiclass-classification/train/examples2\n",
      "Done writing to s3n://kb-sagemaker/sagemaker/DEMO-xgboost-multiclass-classification/train/examples2\n",
      "Writing to s3n://kb-sagemaker/sagemaker/DEMO-xgboost-multiclass-classification/train/examples3\n",
      "Done writing to s3n://kb-sagemaker/sagemaker/DEMO-xgboost-multiclass-classification/train/examples3\n",
      "Writing to s3n://kb-sagemaker/sagemaker/DEMO-xgboost-multiclass-classification/train/examples4\n",
      "Done writing to s3n://kb-sagemaker/sagemaker/DEMO-xgboost-multiclass-classification/train/examples4\n",
      "validation: (10000, 784) (10000,)\n",
      "Writing to s3n://kb-sagemaker/sagemaker/DEMO-xgboost-multiclass-classification/validation/examples0\n",
      "Done writing to s3n://kb-sagemaker/sagemaker/DEMO-xgboost-multiclass-classification/validation/examples0\n",
      "Writing to s3n://kb-sagemaker/sagemaker/DEMO-xgboost-multiclass-classification/validation/examples1\n",
      "Done writing to s3n://kb-sagemaker/sagemaker/DEMO-xgboost-multiclass-classification/validation/examples1\n",
      "Writing to s3n://kb-sagemaker/sagemaker/DEMO-xgboost-multiclass-classification/validation/examples2\n",
      "Done writing to s3n://kb-sagemaker/sagemaker/DEMO-xgboost-multiclass-classification/validation/examples2\n",
      "Writing to s3n://kb-sagemaker/sagemaker/DEMO-xgboost-multiclass-classification/validation/examples3\n",
      "Done writing to s3n://kb-sagemaker/sagemaker/DEMO-xgboost-multiclass-classification/validation/examples3\n",
      "Writing to s3n://kb-sagemaker/sagemaker/DEMO-xgboost-multiclass-classification/validation/examples4\n",
      "Done writing to s3n://kb-sagemaker/sagemaker/DEMO-xgboost-multiclass-classification/validation/examples4\n",
      "test: (10000, 784) (10000,)\n",
      "Writing to s3n://kb-sagemaker/sagemaker/DEMO-xgboost-multiclass-classification/test/examples0\n",
      "Done writing to s3n://kb-sagemaker/sagemaker/DEMO-xgboost-multiclass-classification/test/examples0\n",
      "Writing to s3n://kb-sagemaker/sagemaker/DEMO-xgboost-multiclass-classification/test/examples1\n",
      "Done writing to s3n://kb-sagemaker/sagemaker/DEMO-xgboost-multiclass-classification/test/examples1\n",
      "Writing to s3n://kb-sagemaker/sagemaker/DEMO-xgboost-multiclass-classification/test/examples2\n",
      "Done writing to s3n://kb-sagemaker/sagemaker/DEMO-xgboost-multiclass-classification/test/examples2\n",
      "Writing to s3n://kb-sagemaker/sagemaker/DEMO-xgboost-multiclass-classification/test/examples3\n",
      "Done writing to s3n://kb-sagemaker/sagemaker/DEMO-xgboost-multiclass-classification/test/examples3\n",
      "Writing to s3n://kb-sagemaker/sagemaker/DEMO-xgboost-multiclass-classification/test/examples4\n",
      "Done writing to s3n://kb-sagemaker/sagemaker/DEMO-xgboost-multiclass-classification/test/examples4\n",
      "CPU times: user 38.7 s, sys: 2.2 s, total: 40.9 s\n",
      "Wall time: 46.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "convert_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the XGBoost model\n",
    "\n",
    "Now that we have our data in S3, we can begin training. We'll use Amazon SageMaker XGboost algorithm, and will actually fit two models in order to demonstrate the single machine and distributed training on SageMaker. In the first job, we'll use a single machine to train. In the second job, we'll use two machines and use the ShardedByS3Key mode for the train channel. Since we have 5 part file, one machine will train on three and the other on two part files. Note that the number of instances should not exceed the number of part files. \n",
    "\n",
    "First let's setup a list of training parameters which are common across the two jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "container = get_image_uri(boto3.Session().region_name, 'xgboost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ensure that the train and validation data folders generated above are reflected in the \"InputDataConfig\" parameter below.\n",
    "common_training_params = \\\n",
    "{\n",
    "    \"AlgorithmSpecification\": {\n",
    "        \"TrainingImage\": container,\n",
    "        \"TrainingInputMode\": \"File\"\n",
    "    },\n",
    "    \"RoleArn\": role,\n",
    "    \"OutputDataConfig\": {\n",
    "        \"S3OutputPath\": bucket_path + \"/\"+ prefix + \"/xgboost\"\n",
    "    },\n",
    "    \"ResourceConfig\": {\n",
    "        \"InstanceCount\": 1,   \n",
    "        \"InstanceType\": \"ml.m4.10xlarge\",\n",
    "        \"VolumeSizeInGB\": 5\n",
    "    },\n",
    "    \"HyperParameters\": {\n",
    "        \"max_depth\":\"5\",\n",
    "        \"eta\":\"0.2\",\n",
    "        \"gamma\":\"4\",\n",
    "        \"min_child_weight\":\"6\",\n",
    "        \"silent\":\"0\",\n",
    "        \"objective\": \"multi:softmax\",\n",
    "        \"num_class\": \"10\",\n",
    "        \"num_round\": \"10\"\n",
    "    },\n",
    "    \"StoppingCondition\": {\n",
    "        \"MaxRuntimeInSeconds\": 86400\n",
    "    },\n",
    "    \"InputDataConfig\": [\n",
    "        {\n",
    "            \"ChannelName\": \"train\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": bucket_path + \"/\"+ prefix+ '/train/',\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\" \n",
    "                }\n",
    "            },\n",
    "            \"ContentType\": \"libsvm\",\n",
    "            \"CompressionType\": \"None\"\n",
    "        },\n",
    "        {\n",
    "            \"ChannelName\": \"validation\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": bucket_path + \"/\"+ prefix+ '/validation/',\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "                }\n",
    "            },\n",
    "            \"ContentType\": \"libsvm\",\n",
    "            \"CompressionType\": \"None\"\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create two separate jobs, updating the parameters that are unique to each.\n",
    "\n",
    "### Training on a single instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job name is: DEMO-xgboost-classification2018-11-01-18-26-06\n"
     ]
    }
   ],
   "source": [
    "#single machine job params\n",
    "single_machine_job_name = 'DEMO-xgboost-classification' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(\"Job name is:\", single_machine_job_name)\n",
    "\n",
    "single_machine_job_params = copy.deepcopy(common_training_params)\n",
    "single_machine_job_params['TrainingJobName'] = single_machine_job_name\n",
    "single_machine_job_params['OutputDataConfig']['S3OutputPath'] = bucket_path + \"/\"+ prefix + \"/xgboost-single\"\n",
    "single_machine_job_params['ResourceConfig']['InstanceCount'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on multiple instances\n",
    "\n",
    "You can also run the training job distributed over multiple instances. For larger datasets with multiple partitions, this can significantly boost the training speed. Here we'll still use the small/toy MNIST dataset to demo this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job name is: DEMO-xgboost-distrib-classification2018-11-01-18-26-45\n"
     ]
    }
   ],
   "source": [
    "#distributed job params\n",
    "distributed_job_name = 'DEMO-xgboost-distrib-classification' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(\"Job name is:\", distributed_job_name)\n",
    "\n",
    "distributed_job_params = copy.deepcopy(common_training_params)\n",
    "distributed_job_params['TrainingJobName'] = distributed_job_name\n",
    "distributed_job_params['OutputDataConfig']['S3OutputPath'] = bucket_path + \"/\"+ prefix + \"/xgboost-distributed\"\n",
    "#number of instances used for training\n",
    "distributed_job_params['ResourceConfig']['InstanceCount'] = 2 # no more than 5 if there are total 5 partition files generated above\n",
    "\n",
    "# data distribution type for train channel\n",
    "distributed_job_params['InputDataConfig'][0]['DataSource']['S3DataSource']['S3DataDistributionType'] = 'ShardedByS3Key'\n",
    "# data distribution type for validation channel\n",
    "distributed_job_params['InputDataConfig'][1]['DataSource']['S3DataSource']['S3DataDistributionType'] = 'ShardedByS3Key'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's submit these jobs, taking note that the first will be submitted to run in the background so that we can immediately run the second in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InProgress\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "sm = boto3.Session().client('sagemaker')\n",
    "\n",
    "sm.create_training_job(**single_machine_job_params)\n",
    "sm.create_training_job(**distributed_job_params)\n",
    "\n",
    "status = sm.describe_training_job(TrainingJobName=distributed_job_name)['TrainingJobStatus']\n",
    "print(status)\n",
    "sm.get_waiter('training_job_completed_or_stopped').wait(TrainingJobName=distributed_job_name)\n",
    "status = sm.describe_training_job(TrainingJobName=distributed_job_name)['TrainingJobStatus']\n",
    "print(\"Training job ended with status: \" + status)\n",
    "if status == 'Failed':\n",
    "    message = sm.describe_training_job(TrainingJobName=distributed_job_name)['FailureReason']\n",
    "    print('Training failed with the following error: {}'.format(message))\n",
    "    raise Exception('Training job failed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's confirm both jobs have finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Machine: Completed\n",
      "Distributed: Completed\n"
     ]
    }
   ],
   "source": [
    "print('Single Machine:', sm.describe_training_job(TrainingJobName=single_machine_job_name)['TrainingJobStatus'])\n",
    "print('Distributed:', sm.describe_training_job(TrainingJobName=distributed_job_name)['TrainingJobStatus'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up hosting for the model\n",
    "In order to set up hosting, we have to import the model from training to hosting. The step below demonstrated hosting the model generated from the distributed training job. Same steps can be followed to host the model obtained from the single machine job. \n",
    "\n",
    "### Import model into hosting\n",
    "Next, you register the model with hosting. This allows you the flexibility of importing models trained elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEMO-xgboost-distrib-classification2018-11-01-18-26-45-mod\n",
      "https://s3-us-west-2.amazonaws.com/kb-sagemaker/sagemaker/DEMO-xgboost-multiclass-classification/xgboost-distributed/DEMO-xgboost-distrib-classification2018-11-01-18-26-45/output/model.tar.gz\n",
      "arn:aws:sagemaker:us-west-2:766924284651:model/demo-xgboost-distrib-classification2018-11-01-18-26-45-mod\n",
      "CPU times: user 16.3 ms, sys: 0 ns, total: 16.3 ms\n",
      "Wall time: 377 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import boto3\n",
    "from time import gmtime, strftime\n",
    "\n",
    "model_name=distributed_job_name + '-mod'\n",
    "print(model_name)\n",
    "\n",
    "info = sm.describe_training_job(TrainingJobName=distributed_job_name)\n",
    "model_data = info['ModelArtifacts']['S3ModelArtifacts']\n",
    "print(model_data)\n",
    "\n",
    "primary_container = {\n",
    "    'Image': container,\n",
    "    'ModelDataUrl': model_data\n",
    "}\n",
    "\n",
    "create_model_response = sm.create_model(\n",
    "    ModelName = model_name,\n",
    "    ExecutionRoleArn = role,\n",
    "    PrimaryContainer = primary_container)\n",
    "\n",
    "print(create_model_response['ModelArn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create endpoint configuration\n",
    "SageMaker supports configuring REST endpoints in hosting with multiple models, e.g. for A/B testing purposes. In order to support this, customers create an endpoint configuration, that describes the distribution of traffic across the models, whether split, shadowed, or sampled in some way. In addition, the endpoint configuration describes the instance type required for model deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEMO-XGBoostEndpointConfig-2018-11-01-18-44-11\n",
      "Endpoint Config Arn: arn:aws:sagemaker:us-west-2:766924284651:endpoint-config/demo-xgboostendpointconfig-2018-11-01-18-44-11\n"
     ]
    }
   ],
   "source": [
    "from time import gmtime, strftime\n",
    "\n",
    "endpoint_config_name = 'DEMO-XGBoostEndpointConfig-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(endpoint_config_name)\n",
    "create_endpoint_config_response = sm.create_endpoint_config(\n",
    "    EndpointConfigName = endpoint_config_name,\n",
    "    ProductionVariants=[{\n",
    "        'InstanceType':'ml.m4.xlarge',\n",
    "        'InitialVariantWeight':1,\n",
    "        'InitialInstanceCount':1,\n",
    "        'ModelName':model_name,\n",
    "        'VariantName':'AllTraffic'}])\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response['EndpointConfigArn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create endpoint\n",
    "Lastly, the customer creates the endpoint that serves up the model, through specifying the name and configuration defined above. The end result is an endpoint that can be validated and incorporated into production applications. This takes 9-11 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEMO-XGBoostEndpoint-2018-11-01-18-44-19\n",
      "arn:aws:sagemaker:us-west-2:766924284651:endpoint/demo-xgboostendpoint-2018-11-01-18-44-19\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: InService\n",
      "Arn: arn:aws:sagemaker:us-west-2:766924284651:endpoint/demo-xgboostendpoint-2018-11-01-18-44-19\n",
      "Status: InService\n",
      "CPU times: user 101 ms, sys: 373 µs, total: 102 ms\n",
      "Wall time: 6min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import time\n",
    "\n",
    "endpoint_name = 'DEMO-XGBoostEndpoint-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(endpoint_name)\n",
    "create_endpoint_response = sm.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name)\n",
    "print(create_endpoint_response['EndpointArn'])\n",
    "\n",
    "resp = sm.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp['EndpointStatus']\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status=='Creating':\n",
    "    time.sleep(60)\n",
    "    resp = sm.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp['EndpointStatus']\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp['EndpointArn'])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate the model for use\n",
    "Finally, the customer can now validate the model for use. They can obtain the endpoint from the client library using the result from previous operations, and generate classifications from the trained model using that endpoint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_client = boto3.client('runtime.sagemaker')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to evaluate the model, we'll use the test dataset previously generated. Let us first download the data from S3 to the local host."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from s3n://kb-sagemaker/sagemaker/DEMO-xgboost-multiclass-classification/test/examples0\n"
     ]
    }
   ],
   "source": [
    "download_from_s3('test', 0, 'mnist.local.test') # reading the first part file within test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with a single prediction. Lets use the first record from the test file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -1 mnist.local.test > mnist.single.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label is 7.0.\n",
      "CPU times: user 8.67 ms, sys: 3.91 ms, total: 12.6 ms\n",
      "Wall time: 168 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import json\n",
    "\n",
    "file_name = 'mnist.single.test' #customize to your test file 'mnist.single.test' if use the data above\n",
    "\n",
    "with open(file_name, 'r') as f:\n",
    "    payload = f.read()\n",
    "\n",
    "response = runtime_client.invoke_endpoint(EndpointName=endpoint_name, \n",
    "                                   ContentType='text/x-libsvm', \n",
    "                                   Body=payload)\n",
    "result = response['Body'].read().decode('ascii')\n",
    "print('Predicted label is {}.'.format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 1:0.0 2:0.0 3:0.0 4:0.0 5:0.0 6:0.0 7:0.0 8:0.0 9:0.0 10:0.0 11:0.0 12:0.0 13:0.0 14:0.0 15:0.0 16:0.0 17:0.0 18:0.0 19:0.0 20:0.0 21:0.0 22:0.0 23:0.0 24:0.0 25:0.0 26:0.0 27:0.0 28:0.0 29:0.0 30:0.0 31:0.0 32:0.0 33:0.0 34:0.0 35:0.0 36:0.0 37:0.0 38:0.0 39:0.0 40:0.0 41:0.0 42:0.0 43:0.0 44:0.0 45:0.0 46:0.0 47:0.0 48:0.0 49:0.0 50:0.0 51:0.0 52:0.0 53:0.0 54:0.0 55:0.0 56:0.0 57:0.0 58:0.0 59:0.0 60:0.0 61:0.0 62:0.0 63:0.0 64:0.0 65:0.0 66:0.0 67:0.0 68:0.0 69:0.0 70:0.0 71:0.0 72:0.0 73:0.0 74:0.0 75:0.0 76:0.0 77:0.0 78:0.0 79:0.0 80:0.0 81:0.0 82:0.0 83:0.0 84:0.0 85:0.0 86:0.0 87:0.0 88:0.0 89:0.0 90:0.0 91:0.0 92:0.0 93:0.0 94:0.0 95:0.0 96:0.0 97:0.0 98:0.0 99:0.0 100:0.0 101:0.0 102:0.0 103:0.0 104:0.0 105:0.0 106:0.0 107:0.0 108:0.0 109:0.0 110:0.0 111:0.0 112:0.0 113:0.0 114:0.0 115:0.0 116:0.0 117:0.0 118:0.0 119:0.0 120:0.0 121:0.0 122:0.0 123:0.0 124:0.0 125:0.0 126:0.0 127:0.0 128:0.0 129:0.0 130:0.0 131:0.0 132:0.0 133:0.0 134:0.0 135:0.0 136:0.0 137:0.0 138:0.0 139:0.0 140:0.0 141:0.0 142:0.0 143:0.0 144:0.0 145:0.0 146:0.0 147:0.0 148:0.0 149:0.0 150:0.0 151:0.0 152:0.0 153:0.0 154:0.0 155:0.0 156:0.0 157:0.0 158:0.0 159:0.0 160:0.0 161:0.0 162:0.0 163:0.0 164:0.0 165:0.0 166:0.0 167:0.0 168:0.0 169:0.0 170:0.0 171:0.0 172:0.0 173:0.0 174:0.0 175:0.0 176:0.0 177:0.0 178:0.0 179:0.0 180:0.0 181:0.0 182:0.0 183:0.0 184:0.0 185:0.0 186:0.0 187:0.0 188:0.0 189:0.0 190:0.0 191:0.0 192:0.0 193:0.0 194:0.0 195:0.0 196:0.0 197:0.0 198:0.0 199:0.0 200:0.0 201:0.0 202:0.0 203:0.328125 204:0.72265625 205:0.62109375 206:0.58984375 207:0.234375 208:0.140625 209:0.0 210:0.0 211:0.0 212:0.0 213:0.0 214:0.0 215:0.0 216:0.0 217:0.0 218:0.0 219:0.0 220:0.0 221:0.0 222:0.0 223:0.0 224:0.0 225:0.0 226:0.0 227:0.0 228:0.0 229:0.0 230:0.0 231:0.8671875 232:0.9921875 233:0.9921875 234:0.9921875 235:0.9921875 236:0.94140625 237:0.7734375 238:0.7734375 239:0.7734375 240:0.7734375 241:0.7734375 242:0.7734375 243:0.7734375 244:0.7734375 245:0.6640625 246:0.203125 247:0.0 248:0.0 249:0.0 250:0.0 251:0.0 252:0.0 253:0.0 254:0.0 255:0.0 256:0.0 257:0.0 258:0.0 259:0.26171875 260:0.4453125 261:0.28125 262:0.4453125 263:0.63671875 264:0.88671875 265:0.9921875 266:0.87890625 267:0.9921875 268:0.9921875 269:0.9921875 270:0.9765625 271:0.89453125 272:0.9921875 273:0.9921875 274:0.546875 275:0.0 276:0.0 277:0.0 278:0.0 279:0.0 280:0.0 281:0.0 282:0.0 283:0.0 284:0.0 285:0.0 286:0.0 287:0.0 288:0.0 289:0.0 290:0.0 291:0.0 292:0.06640625 293:0.2578125 294:0.0546875 295:0.26171875 296:0.26171875 297:0.26171875 298:0.23046875 299:0.08203125 300:0.921875 301:0.9921875 302:0.4140625 303:0.0 304:0.0 305:0.0 306:0.0 307:0.0 308:0.0 309:0.0 310:0.0 311:0.0 312:0.0 313:0.0 314:0.0 315:0.0 316:0.0 317:0.0 318:0.0 319:0.0 320:0.0 321:0.0 322:0.0 323:0.0 324:0.0 325:0.0 326:0.0 327:0.32421875 328:0.98828125 329:0.81640625 330:0.0703125 331:0.0 332:0.0 333:0.0 334:0.0 335:0.0 336:0.0 337:0.0 338:0.0 339:0.0 340:0.0 341:0.0 342:0.0 343:0.0 344:0.0 345:0.0 346:0.0 347:0.0 348:0.0 349:0.0 350:0.0 351:0.0 352:0.0 353:0.0 354:0.0859375 355:0.91015625 356:0.99609375 357:0.32421875 358:0.0 359:0.0 360:0.0 361:0.0 362:0.0 363:0.0 364:0.0 365:0.0 366:0.0 367:0.0 368:0.0 369:0.0 370:0.0 371:0.0 372:0.0 373:0.0 374:0.0 375:0.0 376:0.0 377:0.0 378:0.0 379:0.0 380:0.0 381:0.0 382:0.50390625 383:0.9921875 384:0.9296875 385:0.171875 386:0.0 387:0.0 388:0.0 389:0.0 390:0.0 391:0.0 392:0.0 393:0.0 394:0.0 395:0.0 396:0.0 397:0.0 398:0.0 399:0.0 400:0.0 401:0.0 402:0.0 403:0.0 404:0.0 405:0.0 406:0.0 407:0.0 408:0.0 409:0.23046875 410:0.97265625 411:0.9921875 412:0.2421875 413:0.0 414:0.0 415:0.0 416:0.0 417:0.0 418:0.0 419:0.0 420:0.0 421:0.0 422:0.0 423:0.0 424:0.0 425:0.0 426:0.0 427:0.0 428:0.0 429:0.0 430:0.0 431:0.0 432:0.0 433:0.0 434:0.0 435:0.0 436:0.0 437:0.51953125 438:0.9921875 439:0.73046875 440:0.01953125 441:0.0 442:0.0 443:0.0 444:0.0 445:0.0 446:0.0 447:0.0 448:0.0 449:0.0 450:0.0 451:0.0 452:0.0 453:0.0 454:0.0 455:0.0 456:0.0 457:0.0 458:0.0 459:0.0 460:0.0 461:0.0 462:0.0 463:0.0 464:0.03515625 465:0.80078125 466:0.96875 467:0.2265625 468:0.0 469:0.0 470:0.0 471:0.0 472:0.0 473:0.0 474:0.0 475:0.0 476:0.0 477:0.0 478:0.0 479:0.0 480:0.0 481:0.0 482:0.0 483:0.0 484:0.0 485:0.0 486:0.0 487:0.0 488:0.0 489:0.0 490:0.0 491:0.0 492:0.4921875 493:0.9921875 494:0.7109375 495:0.0 496:0.0 497:0.0 498:0.0 499:0.0 500:0.0 501:0.0 502:0.0 503:0.0 504:0.0 505:0.0 506:0.0 507:0.0 508:0.0 509:0.0 510:0.0 511:0.0 512:0.0 513:0.0 514:0.0 515:0.0 516:0.0 517:0.0 518:0.0 519:0.29296875 520:0.98046875 521:0.9375 522:0.22265625 523:0.0 524:0.0 525:0.0 526:0.0 527:0.0 528:0.0 529:0.0 530:0.0 531:0.0 532:0.0 533:0.0 534:0.0 535:0.0 536:0.0 537:0.0 538:0.0 539:0.0 540:0.0 541:0.0 542:0.0 543:0.0 544:0.0 545:0.0 546:0.07421875 547:0.86328125 548:0.9921875 549:0.6484375 550:0.0 551:0.0 552:0.0 553:0.0 554:0.0 555:0.0 556:0.0 557:0.0 558:0.0 559:0.0 560:0.0 561:0.0 562:0.0 563:0.0 564:0.0 565:0.0 566:0.0 567:0.0 568:0.0 569:0.0 570:0.0 571:0.0 572:0.0 573:0.01171875 574:0.79296875 575:0.9921875 576:0.85546875 577:0.13671875 578:0.0 579:0.0 580:0.0 581:0.0 582:0.0 583:0.0 584:0.0 585:0.0 586:0.0 587:0.0 588:0.0 589:0.0 590:0.0 591:0.0 592:0.0 593:0.0 594:0.0 595:0.0 596:0.0 597:0.0 598:0.0 599:0.0 600:0.0 601:0.1484375 602:0.9921875 603:0.9921875 604:0.30078125 605:0.0 606:0.0 607:0.0 608:0.0 609:0.0 610:0.0 611:0.0 612:0.0 613:0.0 614:0.0 615:0.0 616:0.0 617:0.0 618:0.0 619:0.0 620:0.0 621:0.0 622:0.0 623:0.0 624:0.0 625:0.0 626:0.0 627:0.0 628:0.12109375 629:0.875 630:0.9921875 631:0.44921875 632:0.00390625 633:0.0 634:0.0 635:0.0 636:0.0 637:0.0 638:0.0 639:0.0 640:0.0 641:0.0 642:0.0 643:0.0 644:0.0 645:0.0 646:0.0 647:0.0 648:0.0 649:0.0 650:0.0 651:0.0 652:0.0 653:0.0 654:0.0 655:0.0 656:0.51953125 657:0.9921875 658:0.9921875 659:0.203125 660:0.0 661:0.0 662:0.0 663:0.0 664:0.0 665:0.0 666:0.0 667:0.0 668:0.0 669:0.0 670:0.0 671:0.0 672:0.0 673:0.0 674:0.0 675:0.0 676:0.0 677:0.0 678:0.0 679:0.0 680:0.0 681:0.0 682:0.0 683:0.23828125 684:0.9453125 685:0.9921875 686:0.9921875 687:0.203125 688:0.0 689:0.0 690:0.0 691:0.0 692:0.0 693:0.0 694:0.0 695:0.0 696:0.0 697:0.0 698:0.0 699:0.0 700:0.0 701:0.0 702:0.0 703:0.0 704:0.0 705:0.0 706:0.0 707:0.0 708:0.0 709:0.0 710:0.0 711:0.47265625 712:0.9921875 713:0.9921875 714:0.85546875 715:0.15625 716:0.0 717:0.0 718:0.0 719:0.0 720:0.0 721:0.0 722:0.0 723:0.0 724:0.0 725:0.0 726:0.0 727:0.0 728:0.0 729:0.0 730:0.0 731:0.0 732:0.0 733:0.0 734:0.0 735:0.0 736:0.0 737:0.0 738:0.0 739:0.47265625 740:0.9921875 741:0.80859375 742:0.0703125 743:0.0 744:0.0 745:0.0 746:0.0 747:0.0 748:0.0 749:0.0 750:0.0 751:0.0 752:0.0 753:0.0 754:0.0 755:0.0 756:0.0 757:0.0 758:0.0 759:0.0 760:0.0 761:0.0 762:0.0 763:0.0 764:0.0 765:0.0 766:0.0 767:0.0 768:0.0 769:0.0 770:0.0 771:0.0 772:0.0 773:0.0 774:0.0 775:0.0 776:0.0 777:0.0 778:0.0 779:0.0 780:0.0 781:0.0 782:0.0 783:0.0 784:0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, a single prediction works.\n",
    "Let's do a whole batch and see how good is the predictions accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "def do_predict(data, endpoint_name, content_type):\n",
    "    payload = '\\n'.join(data)\n",
    "    response = runtime_client.invoke_endpoint(EndpointName=endpoint_name, \n",
    "                                   ContentType=content_type, \n",
    "                                   Body=payload)\n",
    "    result = response['Body'].read().decode('ascii')\n",
    "    preds = [float(num) for num in result.split(',')]\n",
    "    return preds\n",
    "\n",
    "def batch_predict(data, batch_size, endpoint_name, content_type):\n",
    "    items = len(data)\n",
    "    arrs = []\n",
    "    for offset in range(0, items, batch_size):\n",
    "        arrs.extend(do_predict(data[offset:min(offset+batch_size, items)], endpoint_name, content_type))\n",
    "        sys.stdout.write('.')\n",
    "    return(arrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function helps us calculate the error rate on the batch dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import json\n",
    "\n",
    "file_name = 'mnist.local.test'\n",
    "with open(file_name, 'r') as f:\n",
    "    payload = f.read().strip()\n",
    "\n",
    "labels = [float(line.split(' ')[0]) for line in payload.split('\\n')]\n",
    "test_data = payload.split('\\n')\n",
    "preds = batch_predict(test_data, 100, endpoint_name, 'text/x-libsvm')\n",
    "\n",
    "print ('\\nerror rate=%f' % ( sum(1 for i in range(len(preds)) if preds[i]!=labels[i]) /float(len(preds))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are a few predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the corresponding labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function helps us create the confusion matrix on the labeled batch test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "def error_rate(predictions, labels):\n",
    "    \"\"\"Return the error rate and confusions.\"\"\"\n",
    "    correct = numpy.sum(predictions == labels)\n",
    "    total = predictions.shape[0]\n",
    "\n",
    "    error = 100.0 - (100 * float(correct) / float(total))\n",
    "\n",
    "    confusions = numpy.zeros([10, 10], numpy.int32)\n",
    "    bundled = zip(predictions, labels)\n",
    "    for predicted, actual in bundled:\n",
    "        confusions[int(predicted), int(actual)] += 1\n",
    "    \n",
    "    return error, confusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following helps us visualize the erros that the XGBoost classifier is making. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "NUM_LABELS = 10  # change it according to num_class in your dataset\n",
    "test_error, confusions = error_rate(numpy.asarray(preds), numpy.asarray(labels))\n",
    "print('Test error: %.1f%%' % test_error)\n",
    "\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.grid(False)\n",
    "plt.xticks(numpy.arange(NUM_LABELS))\n",
    "plt.yticks(numpy.arange(NUM_LABELS))\n",
    "plt.imshow(confusions, cmap=plt.cm.jet, interpolation='nearest');\n",
    "\n",
    "for i, cas in enumerate(confusions):\n",
    "    for j, count in enumerate(cas):\n",
    "        if count > 0:\n",
    "            xoff = .07 * len(str(count))\n",
    "            plt.text(j-xoff, i+.2, int(count), fontsize=9, color='white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete Endpoint\n",
    "Once you are done using the endpoint, you can use the following to delete it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.delete_endpoint(EndpointName=endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
